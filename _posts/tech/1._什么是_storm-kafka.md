### 1. 什么是 storm-kafka
#### 1.1 什么是 storm
storm 是一个分布式实时计算系统。storm 的实时计算是相对于 hadoop 等批处理系统的而言的。 storm 适用于如下场景：1. 流式处理， 用来处理源源不断流进来的消息。 2. 分布式 RPC， storm的处理组件是分布式的，而且处理延迟极低，所以可以作为一个通用的分布式rpc框架来使用。

##### 1.1.1 storm 基本概念

* topology: storm中运行的一个实时应用程序，因为各个组件间的消息流动形成逻辑上的一个拓扑结构。

* spout: 在一个topology中产生源数据流的组件。通常情况下spout会从外部数据源中读取数据，然后转换为topology内部的源数据。Spout是一个主动的角色，其接口中有个nextTuple()函数，storm框架会不停地调用此函数，用户只要在其中生成源数据即可。

* bolt: 在一个topology中接受数据然后执行处理的组件。Bolt可以执行过滤、函数操作、合并、写数据库等任何操作。Bolt是一个被动的角色，其接口中有个execute(Tuple input)函数,在接受到消息后会调用此函数，用户可以在其中执行自己想要的操作。

* Tuple: 一次消息传递的基本单元。本来应该是一个key-value的map，但是由于各个组件间传递的tuple的字段名称已经事先定义好，所以tuple中只要按序填入各个value就行了，所以就是一个value list。

##### 1.1.2 storm 记录级容错

### 1.2. 什么是 kafka
#### 1.2.1 partition
#### 1.2.2 offset
#### 1.2.3 消费模型

### 2. storm kafka 简单示例

	public final class KafkaMessageCountTopology {
        public static void main(final String[] args) {
            try {            
                BrokerHosts hosts = new ZkHosts(KAFKA_CLUSTER_PATH);
    
                SpoutConfig conf = new SpoutConfig(hosts, TOPIC_NAME, ZOOKEEPER_OFFSET_STORAGE_PATH, CONSUMER_GROUP_NAME);
    
                conf.scheme = new SchemeAsMultiScheme(new StringScheme());
                conf.zkServers = Lists.newArrayList("localhost");
                conf.zkPort = 2181;
    
                final Config config = new Config();
                config.setMessageTimeoutSecs(60);
                config.setDebug(false);
    
                final TopologyBuilder topologyBuilder = new TopologyBuilder();
    
                topologyBuilder.setSpout("kafkaspout", new KafkaSpout(conf));
                topologyBuilder.setBolt("messagecount", new SplitSentenceBolt())
                        .shuffleGrouping("kafkaspout");
    
                //Submit it to the cluster, or submit it locally
                if (null != args && 0 < args.length) {
                    config.setNumWorkers(3);
                    StormSubmitter.submitTopology(args[0], config, topologyBuilder.createTopology());
                } else {
                    config.setMaxTaskParallelism(10);
                    final LocalCluster localCluster = new LocalCluster();
                    localCluster.submitTopology(TOPOLOGY_NAME, config, topologyBuilder.createTopology());
    
                    Utils.sleep(120 * 1000);
    
                    LOGGER.info("Shutting down the cluster...");
    
                    localCluster.killTopology(TOPOLOGY_NAME);
                    localCluster.shutdown();
                }
            } catch (Exception e) {
                LOGGER.error("Error ...", e);
            }
        }
    
        private static final Logger LOGGER = LoggerFactory.getLogger(KafkaMessageCountTopology.class);
        private static final String TOPOLOGY_NAME = "KafkaMessageCount";
        private static final String KAFKA_ZOOKEEPER_HOST = "localhost:2181";
        private static final String KAFKA_CLUSTER_PATH = KAFKA_ZOOKEEPER_HOST + "/kafka/test-storm-kafka";
        private static final String TOPIC_NAME = "test-storm-kafka";
        private static final String ZOOKEEPER_OFFSET_STORAGE_PATH =  "/offset/" + TOPIC_NAME;
        private static final String CONSUMER_GROUP_NAME =  "test_storm_kaf;
    }	




### 3. kafka-spout 实现分析

基于 kafka 的消费模型， 一个 partition 做多被同一个消费组的一个 consumer 消费，一个 kafka-spout 其实就等同于 consumer， 当启动多个 kakfa-spout 的时候，kafka-spout 需要进行调度，保证一个 partition 只能被一个 kafka-spout 所处理。另一方面 kafka 的 consumer 需要承担管理 offset 的职能，同样 kafka-spout 也需要对 offset 进行管理。

#### 2.1 消费者调度

kafka-spout 的调度有两种: StaticCoordinator 和 ZkCoordinator。简单来说StaticCoordinator 是通过静态配置的方式确定 kafka-spout 消费的 partition， ZkCoordinator 是通过读取 zookeeper 中关于集群的信息来确定 kafka-spout 消费的 parition。

##### 2.1.1 StaticCoordinator

StaticCoordinator 和 ZkCoordinator 在 kafka-spout 中其实就对应两个类，它们均实现同一接口， 我们首先来看下该接口的定义:

	public interface PartitionCoordinator {
    	List<PartitionManager> getMyManagedPartitions();

    	PartitionManager getManager(Partition partition);

    	void refresh();
	}

PartitionManager 即为 Partition 消费的管理类， 主要负责 partition 消息的拉取和 partition offset 的管理。所以不能看出 PartitionCoordinator 的作用主要是给出 kafka-spout 需要消费那些 partition 和 partition 所对应的 broker 地址（确定去哪里拉取数据）。

StaticCoordinator 的实现比较简答，它直接给出 parition 到 broker 地址之间的对应关系，然后由 taskIndex 和 totalTasks 来确定该 kafka-spout 所消费的 parition 以及所在的 broker 地址。 taskIndex 和 totalTasks 由TopologyContext 给出， 每一个 storm 的 spout 都能获取到全局的 topology 信息，它可以知道 spout 的总数目和它自己的 index。

##### 2.1.2 ZkCoordinator

ZkCoordinator 的功能基本等同于 StaticCoordinator， 只是在获取 partition 和 partition -> broker 的映射关系的方式稍有不同。ZkCoordinator 从 zookeeper 获取响应的信息， kafka 的 metadata 信息存储在 zookeeper 上，所以 ZkCoordinator 可以从 zookeeper 上获取到相应的信息。
与 StaticCoordinator 不同的地方是 ZkCoordinator 支持 refresh，即在 partition 信息或者 partition -> broker 的关系发生变化的时候，ZkCoordinator 能够通过刷新感知到这些变化，并及时作出调整。 那么 refresh 在什么情况下会调用呢？

1. 在获取消息失败的时候会调用 offset， 获取消息失败说明 partition 或者 partition -> broker 的关系可能发生了变化，所以需要主动刷新。
2. 在超出 stateUpdateIntervalMs （设置的更新间隔， 默认为 2s），会调用 refresh 进行刷新，也就是会定时刷新。


#### 2.2 offset 管理 

offset 管理的主要包括两方面的内容：1. 消费时的 offset 的提交 2. 启动时的 offset 的获取。storm-spout 的 offset 管理依赖于 zookeeper 进行存储， 提交 offset 的代码如下：

	 long now = System.currentTimeMillis();
        if ((now - _lastUpdateMs) > _spoutConfig.stateUpdateIntervalMs) {
            commit();
        }

可见提交 offset 的策略相对也比较简单，就是定时提交。 提交到 zk 的数据路径如下图：

![结构](storm-kafka-offset)

消费 ID 下有各个 partition 的数据， 每个 partition 存储的数据格式如下：

	{"topology":{"id":"dc2dabab-7965-40c4-a6fe-13d32b91b175"},"offset":4754,"partition":3,"broker":{"host":"127.0.0.1","port":8092},"topic":"topic.test-storm"}

kafka-spout 启动的时候获取 offset 的逻辑如下：

	
	    Long committedTo = null;

        String path = committedPath();
        try {
            Map<Object, Object> json = state.readJSON(path);
            LOG.info("Read partition information from: " + path +  "  --> " + json );
            if (json != null) {
                jsonTopologyId = (String) ((Map<Object, Object>) json.get("topology")).get("id");
                jsonOffset = (Long) json.get("offset");
            }
        } catch (Throwable e) {
            LOG.warn("Error reading and/or parsing at ZkNode: " + path, e);
        }
		// 获取 broker 中的 offset， 可以配置获取最小或者最大的 offset
        Long currentOffset = KafkaUtils.getOffset(consumer, spoutConfig.topic, partition.partition, spoutConfig);
		// 如果从 zookeeper 获取 offset 失败，或者获取的 offset 数据中 TopologyId 不等于当前的 TopologyId， 则将 offset 置为从 broker 获取的 offset
        if (jsonTopologyId == null || jsonOffset == null) { // failed to parse JSON?
            committedTo = currentOffset;
            LOG.info("No partition information found, using configuration to determine offset");
        } else if (!topologyInstanceId.equals(jsonTopologyId) && spoutConfig.ignoreZkOffsets) {
            committedTo = KafkaUtils.getOffset(consumer, spoutConfig.topic, partition.partition, spoutConfig.startOffsetTime);
            LOG.info("Topology change detected and ignore zookeeper offsets set to true, using configuration to determine offset");
        } else {
            committedTo = jsonOffset;
            LOG.info("Read last commit offset from zookeeper: " + committedTo + "; old topology_id: " + jsonTopologyId + " - new topology_id: " + topologyInstanceId );
        }
		// 如果获取的 offset 相对于从 broker 获取的 currentOffset 消息数目大于配置的最大落后消息数目，则重置 offset 为从 broker 获取的 offset
        if (currentOffset - committedTo > spoutConfig.maxOffsetBehind || committedTo <= 0) {
            LOG.info("Last commit offset from zookeeper: " + committedTo);
            Long lastCommittedOffset = committedTo;
            committedTo = currentOffset;
            LOG.info("Commit offset " + lastCommittedOffset + " is more than " +
                    spoutConfig.maxOffsetBehind + " behind latest offset " + currentOffset + ", resetting to startOffsetTime=" + spoutConfig.startOffsetTime);
        }	

从上述代码逻辑总结起来为如下两点：

1. 如果从 zookeeper 获取不到 offset （包括 TopologyId 不匹配的情况）， 则将 offset 设置为从 broker 获取的 offset （可配置为 Earliest 和 Latest）。
2. 如果从 zookeeper 获取的 offset 已经落后配置最大落后数目， 才采用 broker 获取的 offset。

#### 2.3 核心逻辑 nextTuple

一个 spout 最核心的逻辑即为 nextTuple， 即为获取下一个 Tuple 的逻辑， kafka-spout 的代码如下：

	@Override
    public void nextTuple() {
        List<PartitionManager> managers = _coordinator.getMyManagedPartitions();
        for (int i = 0; i < managers.size(); i++) {

            try {
                // in case the number of managers decreased
                _currPartitionIndex = _currPartitionIndex % managers.size();
                EmitState state = managers.get(_currPartitionIndex).next(_collector);
                if (state != EmitState.EMITTED_MORE_LEFT) {
                    _currPartitionIndex = (_currPartitionIndex + 1) % managers.size();
                }
                if (state != EmitState.NO_EMITTED) {
                    break;
                }
            } catch (FailedFetchException e) {
                LOG.warn("Fetch failed", e);
                _coordinator.refresh();
            }
        }

        long now = System.currentTimeMillis();
        if ((now - _lastUpdateMs) > _spoutConfig.stateUpdateIntervalMs) {
            commit();
        }
    }

有了上述关于 coordinator 和 offset 管理的相关知识， 这段代码的逻辑起来就比较容易， 核心逻辑就是从 coordinator 获取消费的所有消费的 partition， 然后采用 round-robin 的方式从一个 partition 拉取一条消息作为下一个 tuple。 在拉取时候的时候刷新 coordinator 中的信息。 提交 offset 的逻辑就是定时提交 offset。

#### 2.4 Ack 机制

storm 通过 ack 机制保证每个 tuple 被 Topology 完全处理。针对 spout-kafka 的情况来说就是当消息处理超时或者有 bolt 处理失败之后需要消息的重新投递。 所以 kakfa-spout 需要记录发射出去的 tuple， 在收到 ack 机制之后才能算是消息处理成功，才能提交该消息的 offset。 消息的 ack 的状况在 PartitionManager 中管理， 采用 SortedMap 进行管理， 当发送一条消息之后将其记录在 SortedMap 中，在消息成功 ack 之后会从 SortedMap 删除该记录。在提交 offset 的时候以 SortedMap 中最小的 firstOffset 作为下一个可以提交的 offset （因为 offset 记录下一个待消费的消息的 offset），这样保证未被消费的消息在下次启动的时候能够被消费，但是也会出现已经被成功消费的消息再次被消费。

![storm-kafka-ack](...)

如上图所示，绿色代表已经收到 ack 的消息， 红色代表还未收到 ack 的消息， SortedMap 记录的数据为 101，105 和 107。那么在提交 offset 的时候为认为目前成功的最大 offset 为 100， 提交 offset 的逻辑如下：

    public long lastCompletedOffset() {
        if (_pending.isEmpty()) {
            return _emittedToOffset;
        } else {
            return _pending.firstKey();
        }
    }

在这种逻辑下会存在一个问题，就是如果这个时候挂掉会造成已经成功消费的 102，103，104 和 106 的重复消费， 如果有一个很小的 offset 的消息由于某种原因始终不能正常消费， 则后续成功消费的消息会在下次启动的时候全部被重新消费， 会造成大量的重复消费逻辑。 kafka-spout 提供了一个配置项 spoutConfig.maxOffsetBehind 可以控制重复消费的数目。 即 ```当前消费 offset - sortMap 中的最小 offset > spoutConfig.maxOffsetBehind``` 时程序会抛出异常。

失败消息的重复消费逻辑比较简单，即在拉取消息的时候优先返回此前处理失败的消息， 具体实现代码见 PartitionManager.fill:

    private void fill() {
        long start = System.nanoTime();
        Long offset;

        // Are there failed tuples? If so, fetch those first.
        offset = this._failedMsgRetryManager.nextFailedMessageToRetry();
        final boolean processingNewTuples = (offset == null);
        if (processingNewTuples) {
            offset = _emittedToOffset;
        }
		...
	

### 3. kafka-bolt 实现分析
//TODO：


http://blog.jassassin.com/2014/10/22/storm/storm-ack/
http://www.searchtb.com/2012/09/introduction-to-storm.html